\section{Requirements}
In preparation for the project the author had multiple informal conversations
with consenting RTI users from different Oxford departments in relation to their
current usage of RTI viewing software and wishes for a modernized software. The prevailing wish was for a unification of the software that is used, most
researchers had to export screenshots from the RTI software, to then add
annotations and drawings in their graphical editing software. But in their
graphical editing software all the advantages of an RTI image were then lost.
Following these discussions, the following requirements were decided upon.
The assumption was made that this project be considered an exploratory piece of work
to show possible new directions for RTI softwares.

The first overall goal is the main focus on web technologies. The new viewer
should be available on as many platforms as possible and allow for easy
distribution and usage, as well as a comfortable developer experience. Because of this, the next priority was a web-supported fileformat. No browser currently supports
RTI or PTM files natively,  so some conversion needs to take place. The
requirement of an additional conversion software would be diametrical to the
`everything web based' goal though, so the browsers need to be able to also
handle the conversion and then support some kind of export. The easiest way to
do so was to develop a newer file format with following features:

\begin{enumerate}
\item Support for embedding/consuming the PTM\cite*{library_of_congress_polynomial_2018}  fileformat.
\item Future support for embedding/consuming the RTI\cite*{library_of_congress_reflectance_2018} fileformat.
\item Extended metadata support.
\item Support for high resolutions.
\item Support for higher bitdepths per pixel than the 8 of PTM/RTI for future
  HDR applications.
\item Easy exchange between multiple researchers and computers.
\end{enumerate}

For the viewer the following features were deemed essential:
\begin{enumerate}[resume]
\item Compatible with all major operating systems and/or web browsers.\label{req_system}
\item Light Controls to change the position of the virtual light source.
\item Multiple rendering modes apart from plain LRGB or RGB.
\item Quick navigation functionality.
\item Annotations to allow for further textual information relating to a part of
  the viewer object.
\item Paintable layers to further clarify surface details.
\end{enumerate}

In addition these non-functional requirements were decided upon:
\begin{enumerate}[resume]
\item  Free Software, the implementation should be available for everyone to
  change and distribute. No email-walling, instead embracing jump-in
  interactions between multiple parties. \label{req_os}
\item Suitable for new developers: for scientists for research purposes,
  or students for educational purposes. \label{req_easy}
\item Plugin support to allow independent additions or modifications to the core software.
\item Good developer experience. \label{req_dx}
\item Adequate performance, at least keeping up with current
  implementations. \label{req_performance}
\item Easy installation for
  researchers. \label{req_install}
\item Fast responses to user interactions. \label{req_react}
\item Reasonable file sizes for instant transfer/viewing.
\item Preservable software and BTF files.\label{req_preserve}
\end{enumerate}

\section{Design}

\subsection{Fileformat}
Although the LoC deems the PTM format ``relatively transparent. It can be viewed
in plain text editors. [\ldots] A very simple program would present these
numbers in a human-readable
form.''\cite*{library_of_congress_polynomial_2018} the author thinks that
another implementation could be even more transparent and easy for people to
understand. One simple inital step is to move to multiple files for representing
the coefficients, each being a valid image (BMP or PNG) themselves. This enables easy examination, simplifies texture loading in a web context (as PNG
textures are supported) and allows easy interactions with other tools. Having
multiple files anyway, the textual data description and metadata can also be
stored in an extra plain text file, making it editable with standard tools. How
to easily distribute multiple files though - just archive (zip) them and distribute as
one. Web browsers do not unpack zips inside their JavaScript run times natively,
but libraries\cite*{noauthor_jszip_nodate} exist for that functionality. All
this multi file handling is abstracted away from the user, they will just handle a
single \emph{.btf.zip} file, but provides access if interested. Having the
ability to add extra files to the archive also allows for easy extensions, for
example the oxrti implementation adds the layer and state saving functionalities, which are stored
as extra entries. For an unpacked example see~\autoref{unpacked}. Having the web
as first target, JSON files are the natural match to describe the contained
data. The data specification's main goal was future extensibility and  coverage
of all existing formats, the developed specification is available standalone at the end of this document
(\autoref{sec_fileformat}) to accommodate easy redistribution and improve
accessibility. One fairly self-explanatory example of converter output:
\begin{json}
{
  "name": "tablet2_LRGB",
  "data": {
    "width": 512,
    "height": 512,
    // the channel model declares how the channels are combined to form the
    resulting picture
    "channelModel": "LRGB",
    "channels": {
      // each channel declares how it should be calculated, RTIpoly6 is the calculated described in the related work section
      "L": {
        "coefficentModel": "RTIpoly6",
        "coefficents": {
          "a0": { "format": "PNG8" },
          "a1": { "format": "PNG8" },
          "a2": { "format": "PNG8" },
          "a3": { "format": "PNG8" },
          "a4": { "format": "PNG8" },
          "a5": { "format": "PNG8" }
        }
      },
      "R": {
        // flat is just taking the straight value from the texture
        "coefficentModel": "flat",
        "coefficents": {
          "a0": { "format": "PNG8" }
        }
      },
      "G": {
        "coefficentModel": "flat",
        "coefficents": {
          "a0": { "format": "PNG8" }
        }
      },
      "B": {
        "coefficentModel": "flat",
        "coefficents": {
          "a0": { "format": "PNG8"}
        }
      }
    },
    // format extra is channelModel dependant and of free JSON form
    "formatExtra": {
      "scales": [2, 2, 2, 2, 2, 2],
      "biases": [149, 151, 68, 77, 77, 0]
    }
  }
}
\end{json}

\fig{unpacked}{Unpacked BTF file}{Unpacked BTF file, manifest.json is the
  standardised data description, oxrti\_* are extensions. The L folder contains
  textures for the single coefficients in grey scale format.}

\subsection{Architecture}
The primary goal for the implementation was to provide many small parts, which
all should be replaceable and flexible to allow the easy addition of any new
wished-for features and the integration of new rendering models. This requires
abstractions in two areas: a flexible plugin system supporting the transparent
integration of new user interfaces and a flexible rendering mechanism, as the
different rendering models should not each be required to implement the viewing
features like panning.

\subsubsection{State Driven Plugins}
The first can be solved as following: Single code units (`Plugins')
containing all their functionality and only integrating with the rest of the
program through an indirect system (`Hooks'), where plugins can declare their
additions to a specific hook - be it a notification hook, where plugins register
a function to be called when an event occurs; or a configuration hook, with
which plugins extend some core functionality with their additions (e.g. adding a
user interface component at some place). The problem of reactive data
exchanges between plugins (e.g. light position change for the rendering
algorithms) and data bindings (for the user interfaces) remains to be solved however.
`Hooking' every variable read and write would result in excessive code bloat
(however, *that would still be a more welcome solution than just writing
variables). Instead an observer system should be used. Variable changes should
be tracked transparently and reading functions/components be notified of changes
automatically.

This proposes the introduction of a state-driven plugin conforming to the
Model-View-Controller pattern. Each plugin should explicitly declare what its
state is (the model), have some specific code changing that state (the
controller) and have one or more user facing components (the view(s)). With this
the observations are one directional because they only go
from state to models, which is far ea easier to track.

For the current oxrti implementation, all Plugins and components are depicted
in~\autoref{classes} and the concrete implementation is shown in~\autoref{sec_plugins}.

\sidefig{classes}{Class and Component Overview}{All classes and components of
  the implementation. Inheritance arrows from each Plugin to the Plugin
  base class have been omitted for sake of  clarity. Boxes are files, usually html/css/glsl.
  Cylinders are rendering stack nodes. Rectangles with two connectors on the
  left are user interface components.}

\subsection{Renderer Stack}
A more traditional frame based rendering stack would likely either re-render
everything every frame (like a realtime game) or fully render all shaders on any input change. But in conjunction with the observation system
above it is possible to implement a more efficient one. Rendering plugins
register themselves at the appropriate hook with a specific priority (highest
rendered first). When a rendering output changes, the successor node inside the
rendering stack is notified of the change and performs a re-rendering on its own.
With this, expansive operations like the model rendering can be cached on lower
levels of the stack. For example the zoom plugin merely transforms these cached outputs.
The concrete implementation is discussed in~\autoref{sec_rendererstack}.